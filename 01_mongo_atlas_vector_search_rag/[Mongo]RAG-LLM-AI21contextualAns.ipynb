{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Retrieval-Augmented Generation: Question Answering based on Custom Dataset\n",
    "\n",
    "Many use cases such as building a chatbot require text (text2text) generation models like **[BloomZ 7B1](https://huggingface.co/bigscience/bloomz-7b1)**, **[Flan T5 XXL](https://huggingface.co/google/flan-t5-xxl)**, and **[Flan T5 UL2](https://huggingface.co/google/flan-ul2)** to respond to user questions with insightful answers. The **BloomZ 7B1**, **Flan T5 XXL**, and **Flan T5 UL2** models have picked up a lot of general knowledge in training, but we often need to ingest and use a large library of more specific information.\n",
    "\n",
    "In this notebook we will demonstrate how to use **AI21 Contextual Answer** to answer questions using a library of documents as a reference, by using document embeddings and retrieval. The embeddings are generated from **GPT-J-6B** embedding model. \n",
    "\n",
    "**This notebook serves a template such that you can easily replace the example dataset by your own to build a custom question and asnwering application.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1. Deploy large language model (LLM) in SageMaker JumpStart\n",
    "\n",
    "To better illustrate the idea, let's first deploy all the models that are required to perform the demo. You can choose either deploying all three Flan T5 XXL, BloomZ 7B1, and Flan UL2 models as the large language model (LLM) to compare their model performances, or select **subset** of the models based on your preference. To do that, you need modify the `_MODEL_CONFIG_` python dictionary defined as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.7/site-packages (4.4.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from pymongo) (2.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker --quiet\n",
    "!pip install ipywidgets==7.0.0 --quiet\n",
    "!pip install --upgrade pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "model_version = \"*\"\n",
    "\n",
    "url = \"https://ep928kbfdd.execute-api.us-east-1.amazonaws.com/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(url, payload):\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json=payload,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def parse_response_model_ai21(query_response):\n",
    "    model_predictions = query_response.json()\n",
    "    generated_text = model_predictions[\"answer\"]\n",
    "    return generated_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please uncomment the entries as below if you want to deploy multiple LLM models to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_MODEL_CONFIG_ = {\n",
    "    # pre-deploy via JS or API Gateway\n",
    "    \"AI21-Contextual-Answers\" : {\n",
    "        \"url\": url,\n",
    "        \"parse_function\": parse_response_model_ai21,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Ask a question to LLM without providing the context\n",
    "\n",
    "To better illustrate why we need retrieval-augmented generation (RAG) based approach to solve the question and anwering problem. Let's directly ask the model a question and see how they respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_index = 0\n",
    "\n",
    "sample = {\n",
    "    0: {\"question\": \"How to scale down SageMaker Asynchronous endpoint to zero?\", \n",
    "        \"context\": \"\"\"You can scale down the Amazon SageMaker Asynchronous Inference endpoint\\\n",
    "instance count to zero in order to save on costs when you are not actively processing\\\n",
    "requests. You need to define a scaling policy that scales on the \"ApproximateBacklogPerInstance\"\\\n",
    "custom metric and set the \"MinCapacity\" value to zero. For step-by-step instructions,\\\n",
    "please visit the `autoscale an asynchronous endpoint` section of the developer guide.\"\"\"},\n",
    "    \n",
    "    1: {\"question\": \"How can I be sure SageMaker protects my data security and privacy?\",\n",
    "        \"context\": \"\"\"Amazon SageMaker does not use or share customer models, training data, or algorithms.\\\n",
    "We know that customers care deeply about privacy and data security. That's why AWS gives\\\n",
    "you ownership and control over your content through simple, powerful tools that allow you\\\n",
    "to determine where your content will be stored, secure your content in transit and at rest,\\\n",
    "and manage your access to AWS services and resources for your users. We also implement\\\n",
    "responsible and sophisticated technical and physical controls that are designed to \\\n",
    "prevent unauthorized access to or disclosure of your content. As a customer, you maintain\\\n",
    "ownership of your content, and you select which AWS services can process, store, \\\n",
    "and host your content. We do not access your content for any purpose without your consent\n",
    "\"\"\"\n",
    "       },\n",
    "    \n",
    "    2: {\"question\": \"Which new GAI features will be launched on SageMaker at 2023?\",\n",
    "        \"context\": \"\"\n",
    "    },\n",
    "    3: {\n",
    "    \"question\": \"What is Amazon SageMaker?\",\n",
    "    \"context\": \"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "question, context = sample[sample_index].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model: AI21-Contextual-Answers, the generated output is:\n",
      "Answer not in document\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"context\": \"\",\n",
    "    \"question\": question\n",
    "}\n",
    "\n",
    "\n",
    "for model_id in _MODEL_CONFIG_:\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        url, payload\n",
    "    )\n",
    "    \n",
    "    generated_texts = _MODEL_CONFIG_[model_id][\"parse_function\"](query_response)\n",
    "    print(f\"For model: {model_id}, the generated output is:\\n{generated_texts}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3. Improve the answer to the same question with insightful context\n",
    "\n",
    "\n",
    "To better answer the question well, we provide extra contextual information, combine it with a prompt, and send it to model together with the question. Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model: AI21-Contextual-Answers, the generated output is:\n",
      "In order to scale down the Amazon SageMaker Asynchronous Inference endpoint instance count to zero, you need to implement a scaling policy that scales on the \"ApproximateBacklogPerInstance\" custom metric and set the \"MinCapacity\" value to zero.\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"context\": context,\n",
    "    \"question\": question\n",
    "}\n",
    "\n",
    "for model_id in _MODEL_CONFIG_:\n",
    "\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        url, payload\n",
    "    )\n",
    "    generated_texts = _MODEL_CONFIG_[model_id][\"parse_function\"](query_response)\n",
    "    print(\n",
    "        f\"For model: {model_id}, the generated output is:\\n{generated_texts}\"\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Use RAG based approach to identify the correct documents, and use them and question to query LLM\n",
    "\n",
    "\n",
    "We plan to use document embeddings to fetch the most relevant documents in our document knowledge library and combine them with the prompt that we provide to LLM.\n",
    "\n",
    "To achieve that, we will do following.\n",
    "\n",
    "* **Generate embedings for each of document in the knowledge library with the GPT-J-6B embedding model.**\n",
    "* **Identify top K most relevant documents based on user query.**\n",
    "    * **For a query of your interest, generate the embedding of the query using the same embedding model.**\n",
    "    * **Search the indexes of top K most relevant documents in the embedding space using the SageMaker KNN algorithm.**\n",
    "    * **Use the indexes to retrieve the corresponded documents.**\n",
    "* **Combine the retrieved documents with prompt and question and send them into LLM.**\n",
    "\n",
    "\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt -- maximum sequence length of 1024 tokens. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4.1 Deploying the model endpoint for All MiniLM L6 v2 embedding model\n",
    "\n",
    "In this section, we will deploy the All MiniLM L6 v2 embedding model from the Jumpstart UI.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "On the left-hand-side navigation pane, got to **Home**, under **SageMaker JumpStart**, choose **Model, notebooks, solutions**. You’re presented with a range of solutions, foundation models, and other artifacts that can help you get started with a specific model or a specific business problem or use case. If you want to experiment in a particular area, you can use the search function. Or you can simply browse the artifacts to find the relevant model or business solution for your needs. To start exploring the Stable Diffusion models, complete the following steps:\n",
    "\n",
    "1. Go to the **Foundation Models** section. In the search bar, search for the **MiniLM L6 v2** model and select the **All MiniLM L6 v2**.\n",
    "<div>\n",
    "    <img src=\"./img/embedding_model.jpg\" alt=\"Image jumpstart\" width=\"800\" style=\"display:inline-block\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "2. A new tab is opened with the options to train, deploy and view model details as shown below. In the Deploy Model section, expand Deployment Configuration. For SageMaker hosting instance, choose the hosting instance (for this lab, we use ml.g5.2xlarge). You can also change the Endpoint name as needed. Then click the Deploy button.\n",
    "\n",
    "<div>\n",
    "    <img src=\"./img/embedding_deploy.jpg\" alt=\"Image deploy\" width=\"600\" style=\"display:inline-block\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "3. The deploy action will start a new tab showing the model creation status and the model deployment status. Wait until the endpoint status shows **In Service**. This will take a few minutes.\n",
    "<div>\n",
    "    <img src=\"./img/ready.jpg\" alt=\"Image ready\" width=\"600\" style=\"display:inline-block\">\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name_embed = \"jumpstart-dft-hf-textembedding-all-minilm-l6-v2\" # change the endpoint name as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def query_endpoint_with_json_payload_endpoint(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    embeddings = model_predictions[\"embedding\"]\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def build_embed_table(df_knowledge, endpoint_name_embed, col_name_4_embed, batch_size=10):\n",
    "    res_embed = []\n",
    "    N = df_knowledge.shape[0]\n",
    "    for idx in tqdm(range(0, N, batch_size)):\n",
    "        content = df_knowledge.loc[idx : (idx + batch_size - 1)][\n",
    "            col_name_4_embed\n",
    "        ].tolist()  ## minus -1 as pandas loc slicing is end-inclusive\n",
    "        payload = {\"text_inputs\": content}\n",
    "        query_response = query_endpoint_with_json_payload_endpoint(\n",
    "            json.dumps(payload).encode(\"utf-8\"), endpoint_name_embed\n",
    "        )\n",
    "        generated_embed = parse_response_multiple_texts(query_response)\n",
    "        res_embed.extend(generated_embed)\n",
    "    res_embed_df = pd.DataFrame(res_embed)\n",
    "    return res_embed_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2. Generate embedings for each of document in the knowledge library with the All MiniLM L6 v2\n",
    " embedding model.\n",
    "\n",
    "For the purpose of the demo we will use [Amazon SageMaker FAQs](https://aws.amazon.com/sagemaker/faqs/) as knowledge library. The data are formatted in a CSV file with two columns Question and Answer. We use **only** the Answer column as the documents of knowledge library, from which relevant documents are retrieved based on a query. \n",
    "\n",
    "**Each row in the CSV format dataset corresponds to a textual document. \n",
    "We will iterate each document to get its embedding vector via the All MiniLM L6 v2 embedding models. \n",
    "For your purpose, you can replace the example dataset of your own to build a custom question and answering application.**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the dataset from our S3 bucket to the local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_path = f\"s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv to ./Amazon_SageMaker_FAQs.csv\n"
     ]
    }
   ],
   "source": [
    "# Downloading the Database\n",
    "!aws s3 cp $s3_path Amazon_SageMaker_FAQs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker is a fully managed service to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For a list of the supported Amazon SageMaker A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon SageMaker is designed for high availabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon SageMaker stores code in ML storage vol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon SageMaker ensures that ML model artifac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amazon SageMaker does not use or share custome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Answer\n",
       "0  Amazon SageMaker is a fully managed service to...\n",
       "1  For a list of the supported Amazon SageMaker A...\n",
       "2  Amazon SageMaker is designed for high availabi...\n",
       "3  Amazon SageMaker stores code in ML storage vol...\n",
       "4  Amazon SageMaker ensures that ML model artifac...\n",
       "5  Amazon SageMaker does not use or share custome..."
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_knowledge = pd.read_csv(\"Amazon_SageMaker_FAQs.csv\", header=None, usecols=[1], names=[\"Answer\"])\n",
    "df_knowledge.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:16<00:00,  2.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.051320</td>\n",
       "      <td>-0.086265</td>\n",
       "      <td>-0.031270</td>\n",
       "      <td>0.033220</td>\n",
       "      <td>0.077008</td>\n",
       "      <td>-0.035504</td>\n",
       "      <td>-0.088567</td>\n",
       "      <td>-0.019618</td>\n",
       "      <td>-0.071139</td>\n",
       "      <td>0.043609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018304</td>\n",
       "      <td>0.028802</td>\n",
       "      <td>0.042111</td>\n",
       "      <td>-0.127226</td>\n",
       "      <td>0.023232</td>\n",
       "      <td>0.040018</td>\n",
       "      <td>0.014618</td>\n",
       "      <td>-0.035364</td>\n",
       "      <td>0.029234</td>\n",
       "      <td>0.015711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062383</td>\n",
       "      <td>-0.073353</td>\n",
       "      <td>-0.024781</td>\n",
       "      <td>-0.019370</td>\n",
       "      <td>0.050714</td>\n",
       "      <td>0.028094</td>\n",
       "      <td>-0.128734</td>\n",
       "      <td>-0.038077</td>\n",
       "      <td>-0.075275</td>\n",
       "      <td>0.017887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.025631</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>-0.131589</td>\n",
       "      <td>0.012769</td>\n",
       "      <td>0.097371</td>\n",
       "      <td>0.093634</td>\n",
       "      <td>-0.007152</td>\n",
       "      <td>-0.047350</td>\n",
       "      <td>0.036941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.077738</td>\n",
       "      <td>-0.078382</td>\n",
       "      <td>-0.029804</td>\n",
       "      <td>0.023051</td>\n",
       "      <td>0.043494</td>\n",
       "      <td>-0.042205</td>\n",
       "      <td>-0.109057</td>\n",
       "      <td>0.015371</td>\n",
       "      <td>-0.021639</td>\n",
       "      <td>0.049813</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028909</td>\n",
       "      <td>-0.002021</td>\n",
       "      <td>0.020902</td>\n",
       "      <td>-0.088226</td>\n",
       "      <td>0.029838</td>\n",
       "      <td>0.029555</td>\n",
       "      <td>0.051726</td>\n",
       "      <td>-0.017517</td>\n",
       "      <td>-0.015522</td>\n",
       "      <td>0.017543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.055477</td>\n",
       "      <td>-0.058904</td>\n",
       "      <td>-0.145159</td>\n",
       "      <td>0.036093</td>\n",
       "      <td>0.061442</td>\n",
       "      <td>0.017926</td>\n",
       "      <td>-0.094529</td>\n",
       "      <td>-0.001391</td>\n",
       "      <td>0.041213</td>\n",
       "      <td>0.031146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018290</td>\n",
       "      <td>0.014639</td>\n",
       "      <td>-0.031307</td>\n",
       "      <td>-0.085898</td>\n",
       "      <td>0.072894</td>\n",
       "      <td>0.011447</td>\n",
       "      <td>0.011394</td>\n",
       "      <td>0.015231</td>\n",
       "      <td>0.036745</td>\n",
       "      <td>-0.059684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.056408</td>\n",
       "      <td>-0.062517</td>\n",
       "      <td>-0.045154</td>\n",
       "      <td>0.006222</td>\n",
       "      <td>0.074342</td>\n",
       "      <td>0.023912</td>\n",
       "      <td>-0.059651</td>\n",
       "      <td>0.008280</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>0.015360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010968</td>\n",
       "      <td>0.053354</td>\n",
       "      <td>0.006876</td>\n",
       "      <td>-0.043502</td>\n",
       "      <td>0.035912</td>\n",
       "      <td>0.059480</td>\n",
       "      <td>0.056548</td>\n",
       "      <td>-0.001481</td>\n",
       "      <td>0.032812</td>\n",
       "      <td>-0.028805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.051320 -0.086265 -0.031270  0.033220  0.077008 -0.035504 -0.088567   \n",
       "1  0.062383 -0.073353 -0.024781 -0.019370  0.050714  0.028094 -0.128734   \n",
       "2 -0.077738 -0.078382 -0.029804  0.023051  0.043494 -0.042205 -0.109057   \n",
       "3 -0.055477 -0.058904 -0.145159  0.036093  0.061442  0.017926 -0.094529   \n",
       "4 -0.056408 -0.062517 -0.045154  0.006222  0.074342  0.023912 -0.059651   \n",
       "\n",
       "        7         8         9    ...       374       375       376       377  \\\n",
       "0 -0.019618 -0.071139  0.043609  ... -0.018304  0.028802  0.042111 -0.127226   \n",
       "1 -0.038077 -0.075275  0.017887  ...  0.005776  0.025631  0.005679 -0.131589   \n",
       "2  0.015371 -0.021639  0.049813  ... -0.028909 -0.002021  0.020902 -0.088226   \n",
       "3 -0.001391  0.041213  0.031146  ... -0.018290  0.014639 -0.031307 -0.085898   \n",
       "4  0.008280  0.029144  0.015360  ... -0.010968  0.053354  0.006876 -0.043502   \n",
       "\n",
       "        378       379       380       381       382       383  \n",
       "0  0.023232  0.040018  0.014618 -0.035364  0.029234  0.015711  \n",
       "1  0.012769  0.097371  0.093634 -0.007152 -0.047350  0.036941  \n",
       "2  0.029838  0.029555  0.051726 -0.017517 -0.015522  0.017543  \n",
       "3  0.072894  0.011447  0.011394  0.015231  0.036745 -0.059684  \n",
       "4  0.035912  0.059480  0.056548 -0.001481  0.032812 -0.028805  \n",
       "\n",
       "[5 rows x 384 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_knowledge_embed = build_embed_table(\n",
    "    df_knowledge, endpoint_name_embed, col_name_4_embed='Answer', batch_size=20\n",
    ")\n",
    "df_knowledge_embed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert df_knowledge_embed.shape[0] == df_knowledge.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the embedding data for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_knowledge_embed.to_csv(\"Amazon_SageMaker_FAQs_embedding.csv\", header=None, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.3. Index the embedding knowledge library using the Mongo Atlas Search Index\n",
    "\n",
    "You can choose to use the Mongo Atlas Search Index, which will conduct following.\n",
    "\n",
    "1. Create a Mongo Atlas database\n",
    "2. Convert document content to embedding\n",
    "3. Insert documents together with its vector to Mongo Atlas collection\n",
    "4. Create vector search indexing from Mongo Atlas collection\n",
    "\n",
    "\n",
    "**Note.** \n",
    "\n",
    "Vector search is a capability that allows you to do semantic search where you are searching data based on meaning. This technique employs machine learning models, often called encoders, to transform text, audio, images, or other types of data into high-dimensional vectors. These vectors capture the semantic meaning of the data, which can then be searched through to find similar content based on vectors being “near” one another in a high-dimensional space\n",
    "\n",
    "You also have other options to store your vectors in a database. [Amazon Opensearch](https://aws.amazon.com/opensearch-service/) is a popular choice for vector DB. You may refer to this [blog](https://aws.amazon.com/blogs/machine-learning/build-a-powerful-question-answering-bot-with-amazon-sagemaker-amazon-opensearch-service-streamlit-and-langchain/) and follow the instructions to build your own RAG solution with Opensearch! Alternatively, you may also use [Amazon Kendra](https://aws.amazon.com/kendra/) for its built-in NLP capabilities and pre-trained domain knowledge. Refer to this [blog](https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/) for guided instructions on how to set it up."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Connect to Mongo Atlas database with pymongo client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# REPLACE ME\n",
    "uri = \"mongodb+srv://admin:csAwr7ZU6Dd3GLLG@awsome-builder.pvlboil.mongodb.net/?retryWrites=true&w=majority\" # REPLACE ME\n",
    "DB_NAME = 'awsome-builder'\n",
    "COLLECTION = \"qna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert documents together with its vector to Mongo Atlas collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed import documents to the Mongo Atlas Collection\n"
     ]
    }
   ],
   "source": [
    "collection = client[DB_NAME][COLLECTION]\n",
    "\n",
    "\n",
    "for index, embedding in enumerate(np.array(df_knowledge_embed)):    \n",
    "    record = { \n",
    "        \"docs\": np.array(df_knowledge)[index][0],\n",
    "        \"plot_embedding\": embedding.tolist() \n",
    "    }\n",
    "    collection.insert_one(record)\n",
    "    \n",
    "print(\"Completed import documents to the Mongo Atlas Collection\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create search indexing from Mongo Atlas collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Atlas Vector Search allows you to search any unstructured data. You can create vector embeddings using the machine learning model of your choice (OpenAI, Hugging Face, and more) and store them in Atlas. It powers use cases such as similarity search, recommendation engines, Q&A systems, dynamic personalization and long-term memory for LLMs.\n",
    "\n",
    "For this workshop, we use Mongo Atlas Search with built-in KNN vector search to get the similarity documents. KNN stands for \"K Nearest Neighbors,\" which is the algorithm frequently used to find vectors near one another."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Create a search index from Mongo Atlas\n",
    "\n",
    "<div>\n",
    "    <img src=\"./img/mongo_search_index.jpg\" alt=\"Image ready\" width=\"600\" style=\"display:inline-block\">\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Mongo Atlas search index json settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```json\n",
    "{\n",
    "    \"mappings\": {\n",
    "    \"dynamic\": true,\n",
    "    \"fields\": {\n",
    "      \"plot_embedding\": {\n",
    "        \"dimensions\": 384,\n",
    "        \"similarity\": \"cosine\",\n",
    "        \"type\": \"knnVector\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "2. Complete search indexing\n",
    "\n",
    "<div>\n",
    "    <img src=\"./img/mongo_search_ready.png\" alt=\"Image ready\" width=\"600\" style=\"display:inline-block\">\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Question to Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to scale down SageMaker Asynchronous endpoint to zero?\n",
      "[[-1.25770820e-02 -2.24030968e-02 -9.01289880e-02  7.80547336e-02\n",
      "  -2.11871639e-02 -3.50977667e-02 -1.00249007e-01  5.81515282e-02\n",
      "  -3.54115926e-02 -3.41335796e-02  3.46331932e-02 -3.09880823e-02\n",
      "  -6.05630726e-02 -5.65665308e-03  1.86302271e-02  3.16554196e-02\n",
      "   1.50426943e-02 -4.70327586e-02 -1.77897848e-02  1.24955783e-02\n",
      "  -4.21567336e-02 -2.70334687e-02  2.69602127e-02  6.11171462e-02\n",
      "   4.77826484e-02 -8.59977305e-02 -5.77164777e-02 -1.69550460e-02\n",
      "   2.61355340e-02 -4.29883935e-02  1.08105034e-01 -4.23644949e-03\n",
      "  -3.97508144e-02 -4.10162518e-03  2.23547332e-02  4.66536209e-02\n",
      "  -3.62018496e-02 -2.69552059e-02  2.05249962e-04  7.39176525e-03\n",
      "   1.28942356e-01  2.86590513e-02 -6.53640553e-02  7.18173897e-03\n",
      "   7.03283474e-02 -2.93456037e-02  6.89646089e-03  2.25637667e-03\n",
      "   2.86780763e-02 -4.96147461e-02  1.62839070e-02  5.43807168e-03\n",
      "  -5.66927567e-02  1.90991759e-02 -5.72254173e-02  5.60955182e-02\n",
      "   8.57988521e-02 -3.28554586e-02 -3.54474448e-02  2.86964560e-03\n",
      "   4.82792929e-02  5.47266193e-03 -6.56438665e-03 -5.00701144e-02\n",
      "   2.60711014e-02  2.93220337e-02  6.29035989e-03 -8.30781907e-02\n",
      "   7.37806549e-03  1.18025042e-01  4.87903804e-02 -6.91138580e-02\n",
      "  -8.31663534e-02  6.01575188e-02  1.12261083e-02  3.27905477e-03\n",
      "  -8.45639929e-02 -2.25245114e-02  1.07787531e-02  3.36750709e-02\n",
      "  -2.72680297e-02  4.16351343e-03 -4.30530421e-02  3.47117218e-03\n",
      "  -3.77702937e-02 -1.12510938e-02  7.54856840e-02  3.80422212e-02\n",
      "   1.20111495e-01 -1.17354123e-02 -6.83574984e-03  4.75078449e-02\n",
      "  -1.67793214e-01 -1.43889885e-03 -4.05927375e-03  2.16453662e-03\n",
      "   5.28473221e-02 -2.56229769e-02  2.92541590e-02  1.52269369e-02\n",
      "   4.60221944e-03 -4.47488055e-02  5.06983660e-02  2.42213719e-02\n",
      "   4.72963005e-02  9.70349461e-02  2.28188536e-03 -1.82581064e-03\n",
      "  -5.96582741e-02 -4.08719145e-02  5.45356013e-02 -3.91167924e-02\n",
      "   3.89376245e-02 -2.80942693e-02  8.35630596e-02  3.87411565e-02\n",
      "   7.58755729e-02  3.23144533e-02 -1.37402236e-01  3.15561444e-02\n",
      "   4.91615683e-02  6.52917996e-02  4.05245759e-02  7.87689462e-02\n",
      "  -3.54567133e-02  7.93907046e-02  3.08181569e-02  1.79590025e-33\n",
      "  -5.74658578e-03 -5.90926558e-02  4.12744097e-02 -4.60861735e-02\n",
      "   5.09951673e-02  7.16895461e-02  1.00790951e-02  4.60418314e-02\n",
      "  -9.14123561e-03 -1.95387620e-02  6.65726140e-02 -3.87527347e-02\n",
      "   1.63706001e-02 -1.35168498e-02 -5.26708774e-02 -7.81658441e-02\n",
      "   1.03695966e-01  6.56595454e-02 -6.94674766e-03 -3.20245326e-02\n",
      "   2.88217664e-02 -6.63179606e-02 -9.68825892e-02 -6.78626588e-03\n",
      "   1.44236712e-02 -3.61366607e-02 -1.71442386e-02  4.72339280e-02\n",
      "  -1.07103810e-01 -1.95847396e-02 -2.68928967e-02 -8.68032873e-03\n",
      "  -3.95238958e-02 -1.02309603e-02 -2.81983037e-02 -1.43645005e-02\n",
      "  -1.19812734e-01 -9.49340127e-03  4.00471576e-02 -4.93590422e-02\n",
      "  -3.44383381e-02  7.40314350e-02 -4.29786853e-02 -9.56216641e-03\n",
      "  -1.09372519e-01 -3.89112420e-02  5.69760948e-02  4.26942296e-02\n",
      "  -1.54080773e-02 -5.70610650e-02 -3.99418399e-02  6.53722063e-02\n",
      "  -7.59460125e-03 -2.76828334e-02  9.52928606e-03 -7.48167187e-02\n",
      "   7.95343220e-02 -3.10107991e-02 -3.62950787e-02 -2.44995281e-02\n",
      "   2.38332339e-02 -6.86983019e-02 -1.27326980e-01 -1.52494051e-02\n",
      "  -8.74871912e-04 -1.64328609e-02 -2.53427234e-02  3.24730203e-02\n",
      "   1.18119651e-02  6.13601953e-02  6.18887180e-03  8.84004831e-02\n",
      "   3.11365887e-03 -1.83561537e-02 -1.49056567e-02 -2.06090137e-02\n",
      "  -2.37843618e-02  5.36308018e-03 -2.06843708e-02 -7.57353231e-02\n",
      "  -1.13956869e-01  1.23029903e-01  1.60063468e-02  8.18975037e-04\n",
      "   1.78899765e-02 -5.93896881e-02  2.02775411e-02  5.28731495e-02\n",
      "  -8.37069303e-02 -2.36745737e-02 -1.84159204e-02  5.68106771e-02\n",
      "   8.18812251e-02  2.54242197e-02 -3.27183083e-02 -1.15234822e-33\n",
      "  -1.98201481e-02 -9.12507880e-04 -2.13551465e-02  8.37615505e-02\n",
      "   3.21532115e-02  7.08280504e-02  1.87017471e-02  5.40699735e-02\n",
      "  -1.74817350e-03  9.92872491e-02  5.53875118e-02  8.42494983e-03\n",
      "  -7.25968480e-02 -4.25345637e-02  7.86269009e-02  1.56154493e-02\n",
      "   4.65797400e-03  4.90478538e-02  3.91160250e-02  1.56180887e-02\n",
      "   4.21853960e-02  2.55418941e-02 -5.82513809e-02  4.06113639e-03\n",
      "   7.40197375e-02 -3.02387346e-02 -5.82761988e-02 -4.19834815e-02\n",
      "  -3.53386067e-03 -2.79982071e-02  8.85264762e-03 -4.49539647e-02\n",
      "  -3.08694188e-02 -5.69302030e-02  5.44577613e-02  3.53507996e-02\n",
      "   1.18236756e-04  6.00657314e-02 -1.07586362e-04  3.49323004e-02\n",
      "   1.17895134e-01  1.55647527e-02 -4.23682742e-02 -2.02258746e-03\n",
      "   3.55612598e-02 -5.54139502e-02 -3.96766476e-02  3.17749158e-02\n",
      "  -7.62962624e-02 -7.63277197e-03 -2.64213122e-02  6.26146188e-03\n",
      "   1.49890047e-03  5.64767085e-02 -5.07760830e-02  2.97083370e-02\n",
      "   8.45961943e-02 -1.85924266e-02 -7.80113321e-03 -7.22998828e-02\n",
      "  -5.66647807e-03 -8.01138133e-02  5.11860326e-02 -1.79354521e-03\n",
      "  -8.84566735e-03 -1.62327792e-02  2.37272531e-02  4.97576334e-02\n",
      "  -3.93728279e-02 -3.31305191e-02  6.42754659e-02  7.31492415e-02\n",
      "   2.89006289e-02  5.53842559e-02 -6.90081790e-02 -3.00612226e-02\n",
      "   2.33184993e-02 -1.06039859e-01  8.77790526e-02 -4.48992550e-02\n",
      "   1.10976957e-02  9.44616040e-04  5.99048994e-02  2.89788041e-02\n",
      "  -2.63780877e-02  1.12591901e-04 -8.22377205e-03  7.85334781e-02\n",
      "   1.89956427e-02  1.00764379e-01 -4.07852009e-02 -3.59602943e-02\n",
      "   6.19521737e-03  1.10776173e-02  6.71033338e-02 -1.80699811e-08\n",
      "  -1.18372232e-01 -1.09992865e-02 -3.15683708e-02  1.42633080e-01\n",
      "  -9.89076588e-03  1.43244583e-02  3.52126127e-03  2.00811382e-02\n",
      "   3.31645012e-02  5.00030816e-02  3.07769291e-02 -6.80200011e-02\n",
      "   3.81051898e-02  5.71580790e-02  6.38265908e-03 -6.20855018e-03\n",
      "   6.82053417e-02  4.46027294e-02  4.96310275e-03 -7.75028318e-02\n",
      "  -7.06620887e-02  2.72351187e-02 -7.65346363e-02 -7.80682489e-02\n",
      "   1.27482554e-03 -6.38308302e-02 -2.76749469e-02  1.64315745e-01\n",
      "   6.62528304e-03 -1.23746663e-01  2.36677602e-02 -2.83054113e-02\n",
      "   1.86134726e-02  1.09924093e-01  1.13327876e-02  4.72588725e-02\n",
      "  -4.80151363e-02  1.21439941e-01  1.99104566e-02  3.38337347e-02\n",
      "  -5.96478879e-02  9.40171350e-03 -2.78613549e-02  1.48008959e-02\n",
      "   2.97839358e-03 -3.07175294e-02 -1.32114133e-02  2.68675499e-02\n",
      "  -1.23734567e-02  6.58383593e-02  6.27480149e-02 -4.86953370e-02\n",
      "   2.92404089e-02 -1.16270827e-02 -1.01317337e-03 -3.78230326e-02\n",
      "  -2.33804230e-02 -2.11430974e-02 -6.53783455e-02  1.03798099e-02\n",
      "   2.79271929e-03 -3.67924646e-02 -8.02872553e-02  2.19752565e-02]]\n"
     ]
    }
   ],
   "source": [
    "query_response = query_endpoint_with_json_payload_endpoint(\n",
    "    question, endpoint_name_embed, content_type=\"application/x-text\"\n",
    ")\n",
    "question_embedding = parse_response_multiple_texts(query_response)\n",
    "print(question, np.array(question_embedding), sep=\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.4 Retrieve the most relevant documents\n",
    "\n",
    "Given the embedding of a query, we will query the endpoint to get the indexes of top K most relevant documents and use the indexes to retrieve the corresponded textual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of relevant document:\n",
      "You can scale down the Amazon SageMaker Asynchronous Inference endpoint instance count to zero in order to save on costs when you are not actively processing requests. You need to define a scaling policy that scales on the \"ApproximateBacklogPerInstance\" custom metric and set the \"MinCapacity\" value to zero. For step-by-step instructions, please visit the autoscale an asynchronous endpoint section of the developer guide. \n"
     ]
    }
   ],
   "source": [
    "K = 1\n",
    "\n",
    "# Query for similar documents.\n",
    "documents = collection.aggregate([\n",
    "    {\n",
    "        \"$search\": {\n",
    "            \"index\": \"default\",\n",
    "            \"knnBeta\": {\n",
    "                \"vector\": question_embedding[0],\n",
    "                \"path\": \"plot_embedding\",\n",
    "                \"k\": K\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "\n",
    "documents = list(documents)\n",
    "\n",
    "context_doc = ''\n",
    "if len(documents) > 0:\n",
    "    context_doc = documents[0]['docs']\n",
    "    \n",
    "print(\"The result of relevant document:\\n{}\".format(context_doc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the retrieved documents, prompt, and question to query the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: You can scale down the Amazon SageMaker Asynchronous Inference endpoint instance count to zero in order to save on costs when you are not actively processing requests. You need to define a scaling policy that scales on the \"ApproximateBacklogPerInstance\" custom metric and set the \"MinCapacity\" value to zero. For step-by-step instructions, please visit the autoscale an asynchronous endpoint section of the developer guide. \n",
      "\n",
      "Answer from model: AI21-Contextual-Answers:\n",
      "In order to scale down the Amazon SageMaker Asynchronous Inference endpoint instance count to zero, you need to define a scaling policy that scales on the \"ApproximateBacklogPerInstance\" custom metric and set the \"MinCapacity\" value to zero.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('context: {}'.format(context_doc))\n",
    "\n",
    "for model_id in _MODEL_CONFIG_:\n",
    "    \n",
    "    if len(context_doc) == 0:\n",
    "        print(f\"Sorry, I don't have information to answer this question:\\n{question}.\")\n",
    "        break\n",
    "    \n",
    "    payload = {\"context\": context_doc, \"question\": question}\n",
    "\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        url, payload\n",
    "    )\n",
    "    generated_texts = _MODEL_CONFIG_[model_id][\"parse_function\"](query_response)\n",
    "    print(f\"\\nAnswer from model: {model_id}:\\n{generated_texts}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Clean up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment below cell to delete the endpoint after testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete the endpoints hosting the embedding model\n",
    "sagemaker_session.delete_endpoint(endpoint_name_embed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model\n",
    "\n",
    "**AI21 Studio Contextual Answers model** allows you to access our high-quality question answering technology. It was designed to answer questions based on a specific document context provided by the customer. This avoids any factual issues that language models may have and makes sure the answers it provides are grounded in that context document.\n",
    "\n",
    "This model receives document text, serving as a context, and a question and returns an answer based entirely on this context. This means that if the answer to your question is not in the document, the model will indicate it (instead of providing a false answer)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of the model's behavior, let's use this toy example of asking what is the Eiffel tower height. Most language models will simply answer according to their training data.\n",
    "\n",
    "This model, however, bases its answer solely on the context you provide. Let's use the following [Wikipedia paragraph](https://en.wikipedia.org/wiki/Eiffel_Tower#:~:text=The%20Eiffel%20Tower%20(%2F%CB%88a%C9%AA,from%20the%20Champ%20de%20Mars) as context, with small modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual paragraph\n",
    "context = \"The tower is 330 metres (1,083 ft) tall,[6] about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest human-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure in the world to surpass both the 200-metre and 300-metre mark in height. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\n",
    "\n",
    "# The paragraph with manual changes of the height\n",
    "false_context = \"The tower is 3 metres (10 ft) tall,[6] about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest human-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure in the world to surpass both the 200-metre and 300-metre mark in height. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\n",
    "\n",
    "# The paragraph with the height omitted\n",
    "partial_context = \"Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest human-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure in the world to surpass both the 200-metre and 300-metre mark in height. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True context\n",
    "payload = {\n",
    "    \"context\": context,\n",
    "    \"question\": \"What is the height of the Eiffel tower?\"\n",
    "}\n",
    "query_endpoint_with_json_payload(url, payload).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# True context\n",
    "payload = {\n",
    "    \"context\": false_context,\n",
    "    \"question\": \"What is the height of the Eiffel tower?\"\n",
    "}\n",
    "query_endpoint_with_json_payload(url, payload).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# True context\n",
    "payload = {\n",
    "    \"context\": partial_context,\n",
    "    \"question\": \"What is the height of the Eiffel tower?\"\n",
    "}\n",
    "query_endpoint_with_json_payload(url, payload).json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ask about financial reports\n",
    "\n",
    "The document context should be **no more than 10,000 characters**, and the question can be up to 160 characters.\n",
    "\n",
    "Imagine you are performing research and rely on financial reports to base your findings. Let's take the following part from [JPMorgan Chase & Co. 2021 annual report](https://www.jpmorganchase.com/content/dam/jpmc/jpmorgan-chase-and-co/investor-relations/documents/annualreport-2021.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "financial_context = \"\"\"In 2020 and 2021, enormous QE — approximately $4.4 trillion, or 18%, of 2021 gross domestic product (GDP) — and enormous fiscal stimulus (which has been and always will be inflationary) — approximately $5 trillion, or 21%, of 2021 GDP — stabilized markets and allowed companies to raise enormous amounts of capital. In addition, this infusion of capital saved many small businesses and put more than $2.5 trillion in the hands of consumers and almost $1 trillion into state and local coffers. These actions led to a rapid decline in unemployment, dropping from 15% to under 4% in 20 months — the magnitude and speed of which were both unprecedented. Additionally, the economy grew 7% in 2021 despite the arrival of the Delta and Omicron variants and the global supply chain shortages, which were largely fueled by the dramatic upswing in consumer spending and the shift in that spend from services to goods. Fortunately, during these two years, vaccines for COVID-19 were also rapidly developed and distributed.\n",
    "In today's economy, the consumer is in excellent financial shape (on average), with leverage among the lowest on record, excellent mortgage underwriting (even though we've had home price appreciation), plentiful jobs with wage increases and more than $2 trillion in excess savings, mostly due to government stimulus. Most consumers and companies (and states) are still flush with the money generated in 2020 and 2021, with consumer spending over the last several months 12% above pre-COVID-19 levels. (But we must recognize that the account balances in lower-income households, smaller to begin with, are going down faster and that income for those households is not keeping pace with rising inflation.)\n",
    "Today's economic landscape is completely different from the 2008 financial crisis when the consumer was extraordinarily overleveraged, as was the financial system as a whole — from banks and investment banks to shadow banks, hedge funds, private equity, Fannie Mae and many other entities. In addition, home price appreciation, fed by bad underwriting and leverage in the mortgage system, led to excessive speculation, which was missed by virtually everyone — eventually leading to nearly $1 trillion in actual losses.\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than reading the entire report, just ask what you want to know. The model will answer based on the provided report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"context\": financial_context,\n",
    "    \"question\": \"Did the economy shrink after the Omicron variant arrived?\"\n",
    "}\n",
    "query_endpoint_with_json_payload(url, payload).json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, you can ask more complex questions, where the answer requires deductions rather than just extracting the correct sentence from the document context. This will result in abstractive, rather than extractive, answers that draw on several different parts of the document. For example, look at the following question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"context\": financial_context,\n",
    "    \"question\":  \"Did COVID-19 eventually help the economy?\"\n",
    "}\n",
    "query_endpoint_with_json_payload(url, payload).json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now present the model with the following question. You may be confused to answer something based on the last paragraph without delving into the text. However, if you read the provided document context properly, you will discover that the answer does not appear there. The model handles this as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"context\": financial_context,\n",
    "    \"question\":  \"How did COVID-19 affect the financial crisis of 2008?\"\n",
    "}\n",
    "query_endpoint_with_json_payload(url, payload).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
